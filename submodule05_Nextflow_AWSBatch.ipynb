{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99cfa998-06b6-4b42-ae3a-b4e011750d31",
   "metadata": {},
   "source": [
    "# Genome Assembly and Assessment using Nextflow and AWS Batch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c090b42f-ffd5-41c0-b599-9e1f8a463369",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This short tutorial demonstrates how to run a comparative genomics workflow using a bacteria data set. Steps in the workflow combine the analyses you performed in previous submodules, and include:\n",
    "- Read QC and trimming (fastqc, fastp)\n",
    "- Genome assembly (SPAdes)\n",
    "- Contiguity assessment (QUAST)\n",
    "- Completeness assessment (BUSCO)\n",
    "- Coverage assessment (BWA & Samtools)\n",
    "- Taxonomic assignment (BLAST)\n",
    "- Combine datasets (Blobtools)\n",
    "- Genome annotation (Bakta)\n",
    "\n",
    "The tutorial uses a popular workflow manager called [Nextflow](https://www.nextflow.io) run via [AWS Batch](https://aws.amazon.com/batch/) to automate the processes run in Submodules 1 & 2.\n",
    "\n",
    "\n",
    "#### About AWS Batch\n",
    "AWS Batch will create the needed permissions, roles and resources to run Nextflow in a serverless manner. You can set up AWS Batch manually or deploy it **automatically** with a stack template. Please see **Setting up AWS Batch** in the Get Started section below to learn more about how to use it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e981d2-010c-4139-80d5-7e7c4b14f8a5",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "#### Python requirements\n",
    "+ Python >= 3.8\n",
    "\n",
    "#### AWS requirements\n",
    "+ Please ensure you have a VPC, subnets, and security group set up before running this tutorial.\n",
    "+ Role with AdministratorAccess, AmazonSageMakerFullAccess, S3 access and AWSBatchServiceRole.\n",
    "+ Instance Role with AmazonECS_FullAccess, AmazonEC2ContainerRegistryFullAccess, and S3 access.\n",
    "+ If you do not have the required set-up for AWS Batch please follow this tutorial [here](https://github.com/STRIDES/NIHCloudLabAWS/blob/main/notebooks/AWSBatch/Intro_AWS_Batch.ipynb).\n",
    "+ ***When making the instance role, make another for SageMaker notebooks with the following permissions: AdminstratorAccess, AmazonEC2ContainerRegistryFullAccess, AmazonECS_FullAccess, AmazonS3FullAccess, AmazonSageMakerFullAccess, and AWSBatchServiceRole.***\n",
    "+ It is recommended that specific permission to folders are added through inline policy. An example of the JSON is below:\n",
    "\n",
    "<pre>\n",
    "{\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Sid\": \"AllowSageMakerS3Access\",\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"s3:GetObject\",\n",
    "                \"s3:PutObject\",\n",
    "                \"s3:ListBucket\",\n",
    "                \"s3:GetBucketLocation\",\n",
    "                \"s3:CreateBucket\"\n",
    "            ],\n",
    "            \"Resource\": [\n",
    "                \"arn:aws:s3:::batch-bucket\",\n",
    "                \"arn:aws:s3:::batch-bucket/*\",\n",
    "                \"arn:aws:s3:::nigms-sandbox-healthomics\",\n",
    "                \"arn:aws:s3:::nigms-sandbox-healthomics/*\",\n",
    "                \"arn:aws:s3:::ngi-igenomes\",\n",
    "                \"arn:aws:s3:::ngi-igenomes/*\"\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "</pre>\n",
    "For AWS bucket naming conventions, please click [here](https://docs.aws.amazon.com/AmazonS3/latest/userguide/bucketnamingrules.html).\n",
    "\n",
    "\n",
    "\n",
    "<div style=\"border: 1px solid #ffe69c; padding: 0px; border-radius: 4px;\">\n",
    "  <div style=\"background-color: #fff3cd; padding: 5px; font-weight: bold;\">\n",
    "    <i class=\"fas fa-exclamation-triangle\" style=\"color: #664d03;margin-right: 5px;\"></i><a style=\"color: #664d03\">Before using AWS Batch </a>\n",
    "  </div>\n",
    "  <p style=\"margin-left: 5px;\">\n",
    "Before begining this tutorial, if you do not have required roles, policies, permissions or compute environment and would like to <b>manually</b> set those up please click <a href=\"https://github.com/NIGMS/NIGMS-Sandbox/blob/main/docs/AWS-Batch-Setup.md\">here</a> to set that up.\n",
    "  </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ebe2e1f-1dc1-44a3-a0e3-f5fac1dac374",
   "metadata": {},
   "source": [
    "## Get Started\n",
    "### Step 0. Setting up AWS Batch\n",
    "AWS Batch manages the provisioning of compute environments (EC2, Fargate), container orchestration, job queues, IAM roles, and permissions. We can deploy a full environment either:\n",
    "- Automatically using a preconfigured AWS CloudFormation stack (**recommended**)\n",
    "- Manually by setting up roles, queues, and buckets\n",
    "The Launch Stack button below will take you to the cloud formation create stack webpage with the template with required resources already linked. \n",
    "\n",
    "If you prefer to skip manual deployment and deploy automatically in the cloud, click the **Launch Stack** button below. For a walkthrough of the screens during automatic deployment please click [here](https://github.com/NIGMS/NIGMS-Sandbox/blob/main/docs/HowToLaunchAWSBatch.md). The deployment should take ~5 min and then the resources will be ready for use. \n",
    "\n",
    "[![Launch Stack](images/LaunchStack.jpg)](https://console.aws.amazon.com/cloudformation/home?#/stacks/new?stackName=aws-batch-nigms&templateURL=https://nigms-sandbox.s3.us-east-1.amazonaws.com/cf-templates/AWSBatch_template.yaml )\n",
    "\n",
    "### Step 1. Install dependencies, update paths and create a new S3 Bucket to store input and output files\n",
    "After setting up a AWS CloudFormation stack, need to let the nextflow workflow to know where are those resrouces by providing the configuration:\n",
    "<div style=\"border: 1px solid #e57373; padding: 0px; border-radius: 4px;\">\n",
    "  <div style=\"background-color: #ffcdd2; padding: 5px; \">\n",
    "    <i class=\"fas fa-exclamation-triangle\" style=\"color: #b71c1c;margin-right: 5px;\"></i><a style=\"color: #b71c1c\"><b>Important</b> - Customize Required</a>\n",
    "  </div>\n",
    "  <p style=\"margin-left: 5px;\">\n",
    "After successfull creation of your stack you must attatch a new role to SageMaker to be able to submit batch jobs. Please following the the following steps to change your SageMaker role:<br>\n",
    "<ol> <li>Navigate to your SageMaker AI notebook dashboard (where you initially created and launched your VM)</li> <li>Locate your instance and click the <b>Stop</b> button</li> <li>Once the instance is stopped: <ul> <li>Click <b>Edit</b></li> <li>Scroll to the \"Permissions and encryption\" section</li> <li>Click the IAM role dropdown</li> <li>Select the new role created during stack formation (named something like <b>aws-batch-nigms-SageMakerExecutionRole</b>)</li> </ul> </li> \n",
    "<li>Click <b>Update notebook instance</b> to save your changes</li> \n",
    "<li>After the update completes: <ul> <li>Click <b>Start</b> to relaunch your instance</li> <li>Reconnect to your instance</li> <li>Resume your work from this point</li> </ul> </li> </ol>\n",
    "\n",
    "<b>Warning:</b> Make sure to replace the <b>stack name</b> to the stack that you just created. <code>STACK_NAME = \"your-stack-name-here\"</code>\n",
    "  </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536d98ed-54f8-43a8-aeeb-08859f85f1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfine a stack name variable\n",
    "STACK_NAME = \"aws-batch-nigms-test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25ee32c-644e-4935-b2d6-7f0effcb2e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "# Get account ID and region \n",
    "account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
    "region = boto3.session.Session().region_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa828d0a-388e-487b-b8d5-b957774c153b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set variable names \n",
    "# These variables should come from the Intro AWS Batch tutorial (or leave as-is if using the launch stack button)\n",
    "BUCKET_NAME = f\"{STACK_NAME}-batch-bucket-{account_id}\"\n",
    "AWS_QUEUE = f\"{STACK_NAME}-JobQueue\"\n",
    "INPUT_FOLDER = 'nigms-sandbox/unh-wgsbac-pipeline'\n",
    "AWS_REGION = region"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4ebe7d-e5e4-4ef4-a4da-61a7d7b05984",
   "metadata": {},
   "source": [
    "#### Install dependencies\n",
    "Installs Nextflow and Java, which are required to execute the pipeline. In environments like SageMaker, Java is usually pre-installed. But if you're running outside SageMaker (e.g., EC2 or local), you’ll need to manually install it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5defbe3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Nextflow\n",
    "! mamba install -y -c conda-forge -c bioconda nextflow --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9309e19c-440d-4d33-9813-087ba645f527",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Install Java and Nextflow if needed in other systems</summary>\n",
    "If using other system other than AWS SageMaker Notebook, you might need to install java and nextflow using the code below:\n",
    "<br> <i># Install java</i><pre>\n",
    "    sudo apt update\n",
    "    sudo apt-get install default-jdk -y\n",
    "    java -version\n",
    "    </pre>\n",
    "    <i># Install Nextflow</i><pre>\n",
    "    curl https://get.nextflow.io | bash\n",
    "    chmod +x nextflow\n",
    "    ./nextflow self-update\n",
    "    ./nextflow plugin update\n",
    "    </pre>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d3c2cf-c12c-4343-871e-14e147d79929",
   "metadata": {},
   "source": [
    "#### Create additional .config file needed\n",
    "A configuration template (<code>aws_batch_template.config</code>) is customized below with your actual AWS values:\n",
    "- S3 bucket name\n",
    "- AWS job queue name\n",
    "- AWS region\n",
    "\n",
    "This file tells Nextflow later how to communicate with AWS Batch and where to find the resources it needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c9fb02-dbe9-41d2-a984-7634f6aebd91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy the aws batch configuration file \n",
    "! cp wgsbac/conf/aws_batch_template.config aws_batch_submodule5.config \n",
    "# replace batch bucket name in nextflow configuration file\n",
    "! sed -i \"s/aws-batch-nigms-batch-bucket-/$BUCKET_NAME/g\" aws_batch_submodule5.config \n",
    "# replace job queue name in configuration file \n",
    "! sed -i \"s/aws-batch-nigms-JobQueue/$AWS_QUEUE/g\" aws_batch_submodule5.config \n",
    "# replace the region placeholder with your region\n",
    "! sed -i \"s/us-east-1/$AWS_REGION/g\" aws_batch_submodule5.config "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9cbd3e-d003-48d1-8b41-924c74b9486f",
   "metadata": {},
   "source": [
    "### Step 2. Enable AWS Batch for the nextflow script \n",
    "Run the pipeline in a cloud-native, serverless manner using AWS Batch. AWS Batch offloads the burden of provisioning and managing compute resources. When you execute this command:\n",
    "- Nextflow uploads tasks to AWS Batch. \n",
    "- AWS Batch pulls the necessary containers.\n",
    "- Each process/task in the pipeline runs as an isolated job in the cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6cb4617-b98c-41b6-995d-711d2f722cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run nextflow script with parameters \n",
    "#! ./nextflow run wgsbac/main.nf --input s3://$INPUT_FOLDER/samplesheet_test.csv -profile docker,awsbatch -c wgsbac/nextflow.config --awsqueue $AWS_QUEUE --awsregion $AWS_REGION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065b7e42-53f4-4cd9-a067-d03c7b87f482",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run nextflow script with parameters \n",
    "! nextflow run wgsbac/main.nf -profile docker,awsbatch \\\n",
    "    --input s3://$INPUT_FOLDER/samplesheet_test.csv \\\n",
    "    -c aws_batch_submodule5.config \\\n",
    "    -resume"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309fff1d-9c02-4386-895d-787fb8c3b5b4",
   "metadata": {},
   "source": [
    "#### Key Differences from Local Execution:\n",
    "\n",
    "<table border=\"1\" cellpadding=\"8\" cellspacing=\"0\">\n",
    "  <thead>\n",
    "    <tr>\n",
    "      <th>Feature</th>\n",
    "      <th>Local Execution (e.g., SageMaker)</th>\n",
    "      <th>AWS Batch Execution</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <td><strong>Compute</strong></td>\n",
    "      <td>Uses notebook’s limited CPU/memory</td>\n",
    "      <td>Uses scalable EC2/Fargate resources</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td><strong>Data Location</strong></td>\n",
    "      <td>Reads from local disk</td>\n",
    "      <td>Reads directly from S3</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td><strong>Job Management</strong></td>\n",
    "      <td>Manual or single-threaded</td>\n",
    "      <td>Distributed via job queues</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td><strong>Reproducibility</strong></td>\n",
    "      <td>Depends on local environment</td>\n",
    "      <td>Fully containerized via Docker</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td><strong>Fault Tolerance</strong></td>\n",
    "      <td>Limited</td>\n",
    "      <td>Retries & logs handled by AWS Batch</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e18731d-59ef-4c45-bc9e-6e5773f1b786",
   "metadata": {},
   "source": [
    "### Step 3: Explore Results\n",
    "This command lets you preview the full paths of output files within the S3 bucket. These results should be identical to the ones generated in Submodule 4, where the pipeline was run locally. The only differences lie in: (1) Execution environment: Local notebook vs. AWS Batch; (2)Data paths: Local file system vs. S3 input/output directories. Everything else: tools, parameters, and pipeline structure, remains the same, ensuring consistency across both local and cloud executions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db39f248-77c0-4c98-8046-ac78a401c729",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View output files that were output to S3 bucket\n",
    "! aws s3 ls s3://$BUCKET_NAME/nextflow_output/final_reports/final_results/ --recursive | cut -c32-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5d0630-1d85-4625-bc04-036aae11ce4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy output to local results folder (same outdir as if workflow was run locally)\n",
    "! aws s3 sync s3://$BUCKET_NAME/nextflow_output/final_reports/ wgsbac/assets/results/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7547218-82bb-4f77-870f-1daa52feceed",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# view contents of local output directory\n",
    "ls wgsbac/assets/results/final_results/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16777a14-dc64-4e35-a583-71259021ac33",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# view annotated proteome file(s) output by bakta\n",
    "ls wgsbac/assets/results/final_results/bakta_results/proteomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6852bea-adb3-4570-b3d2-172c518c8da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# view blobtools plots\n",
    "from IPython.display import Image\n",
    "Image('wgsbac/assets/results/final_results/blobtools_plots/SRR10056829_T1_blobplot.png', width=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8b62cc-e9f9-4e10-9037-514df6900f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image('wgsbac/assets/results/final_results/blobtools_plots/SRR10056829_T1_blobplot_read_cov.png', width=1200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb75509d-3000-4cfd-93ac-d910d2a97dca",
   "metadata": {},
   "source": [
    "## Clean Up the AWS Environment\n",
    "\n",
    "Once you've successfully run your analysis and downloaded the results, it's a good idea to clean up unused resources to avoid unnecessary charges.\n",
    "\n",
    "#### Recommended Cleanup Steps:\n",
    "\n",
    "- **Delete Output Files from S3 (Optional)**  \n",
    "    If you've downloaded your results locally and no longer need them stored in the cloud.\n",
    "- **Delete the S3 Bucket (Optional)**    \n",
    "  To remove the entire bucket (only do this if you're sure!)\n",
    "- **Shut Down AWS Batch Resources (Optional but Recommended):**    \n",
    "  If you used a CloudFormation stack to set up AWS Batch, you can delete all associated resources in one step (⚠️ Note: Deleting the stack will also remove IAM roles and compute environments created by the template.):\n",
    "  + Go to the <a href=\"https://console.aws.amazon.com/cloudformation/\">AWS CloudFormation Console</a>\n",
    "  + Select your stack (e.g., <code>aws-batch-nigms-test1</code>)\n",
    "  + Click Delete\n",
    "  + Wait for all resources (compute environments, roles, queues) to be removed\n",
    "  \n",
    "<div style=\"border: 1px solid #659078; padding: 0px; border-radius: 4px;\">\n",
    "  <div style=\"background-color: #d4edda; padding: 5px; font-weight: bold;\">\n",
    "    <i class=\"fas fa-lightbulb\" style=\"color: #0e4628;margin-right: 5px;\"></i><a style=\"color: #0e4628\">Tips</a>\n",
    "  </div>\n",
    "  <p style=\"margin-left: 5px;\">\n",
    "It’s always good practice to periodically review your <b>EC2 instances</b>, <b>ECR containers</b>, <b>S3 storage</b>, and <b>CloudWatch logs</b> to ensure no stray resources are incurring charges.\n",
    "  </p>\n",
    "</div>\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
